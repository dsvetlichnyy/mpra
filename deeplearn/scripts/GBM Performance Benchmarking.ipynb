{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/rmovva/anaconda2/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-mer XGBoost on Sharpr\n",
    "Get all k-mers (k from 1 to X, where X = 6/7/8 depending on training capabilities), use as features to train XGB on Sharpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new method took 0.010\n",
      "(1, 5460)\n",
      "27\n",
      "33 0.0925825534504\n",
      "\n",
      "0011111001001110\n",
      "old method took 0.001\n",
      "(1, 5460)\n",
      "21\n",
      "33 0.113873322596\n",
      "39\n",
      "[ 3  4  7  8  9 13 14 15 18 34]\n",
      "[3 0 2 1 0 1 0 0 2 1]\n",
      "[5 1 0 0 1 0 3 2 0 0]\n",
      "8\n",
      "10\n",
      "7\n",
      "8\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "def get_seqs_from_file(fasta):\n",
    "    seqfile = gzip.open(fasta)\n",
    "    seqs = {}\n",
    "    for line in seqfile:\n",
    "        if line[0] == '>':\n",
    "            name = line.strip()[1:]\n",
    "        else:\n",
    "            seq = line.strip()\n",
    "            seqs[name] = seq\n",
    "    seqfile.close()\n",
    "    return seqs\n",
    "\n",
    "def get_labels_from_file(filename):\n",
    "    labelfile = gzip.open(filename)\n",
    "    labels = {}\n",
    "    labelfile.readline()\n",
    "    for line in labelfile:\n",
    "        line = line.strip().split('\\t')\n",
    "        name = line[0]\n",
    "        vals = [float(lbl) for lbl in line[13:]] # 1 for name + 12 for counts --> 13:\n",
    "        labels[name] = vals\n",
    "    labelfile.close()\n",
    "    return labels\n",
    "        \n",
    "\n",
    "def get_seqs_labels_from_split(filename, seqs, labels):\n",
    "    splitFile = gzip.open(filename)\n",
    "    seqs_from_split = []\n",
    "    labels_from_split = []\n",
    "    for line in splitFile:\n",
    "        name = line.strip()\n",
    "        seqs_from_split.append(seqs[name])\n",
    "        labels_from_split.append(labels[name])\n",
    "    splitFile.close()\n",
    "    return np.array(seqs_from_split), np.array(labels_from_split)\n",
    "\n",
    "# Functions adapted from Joe Paggi (https://github.com/jpaggi/deepmpra/blob/master/models/kmer_model.py)\n",
    "\n",
    "BASES = ['A', 'C', 'G', 'T']\n",
    "\n",
    "def seqs_to_matrix(seqs):\n",
    "    return np.vstack([map(lambda x: BASES.index(x), seq)\n",
    "                     for seq in seqs])\n",
    "\n",
    "def get_kmer_features(seqs, k):\n",
    "    X = seqs_to_matrix(seqs)\n",
    "    bases = ['00', '01', '10', '11']\n",
    "    counts = []\n",
    "    for seq in X:\n",
    "        binary_seq = ''.join(map(lambda x: bases[x], seq))\n",
    "        print binary_seq\n",
    "        k_vals = np.arange(1, k+1)\n",
    "        count = np.zeros(np.sum(map(lambda x: 4**x, k_vals)), dtype = np.uint8)\n",
    "        count_idx = 0\n",
    "        for k_val in k_vals:\n",
    "            for i in range(0, len(seq) - k_val + 1):\n",
    "                count[int(binary_seq[i*2:(i+k_val)*2], 2)] += 1\n",
    "        counts += [count]\n",
    "    return np.vstack(counts)\n",
    "\n",
    "def get_kmer_features_strings(seqs, k):\n",
    "    feature_matrix = []\n",
    "    kmers = []\n",
    "    for i in range(1, k+1):\n",
    "        kmers += [''.join(kmer) for kmer in list(itertools.product(*[BASES for strlen in range(i)]))]\n",
    "    for seq in seqs:\n",
    "        k_vals = np.arange(1, k+1)\n",
    "        kmer_counts = OrderedDict()\n",
    "        for kmer in kmers:\n",
    "            kmer_counts[kmer] = 0\n",
    "        for i in range(0, len(seq)):\n",
    "            for kmer_len in range(1, k+1):\n",
    "                if i + kmer_len > len(seq):\n",
    "                    continue\n",
    "                kmer_counts[seq[i : i+kmer_len]] += 1\n",
    "        feature_matrix.append([kmer_counts[kmer] for kmer in kmer_counts])\n",
    "    return np.array(feature_matrix)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "X_tst1 = get_kmer_features_strings(['ATTGCATG'], 6)\n",
    "print \"new method took %.3f\" % (time.time() - t0)\n",
    "print X_tst1.shape\n",
    "cnt = 0\n",
    "for i in range(X_tst1.shape[1]):\n",
    "    if np.mean(X_tst1[:, i]) != 0:\n",
    "        cnt += 1\n",
    "print cnt\n",
    "print np.sum(X_tst1), np.std(X_tst1)\n",
    "\n",
    "print\n",
    "\n",
    "t0 = time.time()\n",
    "X_tst2 = get_kmer_features(['ATTGCATG'], 6)\n",
    "print \"old method took %.3f\" % (time.time() - t0)\n",
    "print X_tst2.shape\n",
    "cnt = 0\n",
    "for i in range(X_tst2.shape[1]):\n",
    "    if np.mean(X_tst2[:, i]) != 0:\n",
    "        cnt += 1\n",
    "print cnt\n",
    "print np.sum(X_tst2), np.std(X_tst2)\n",
    "\n",
    "idxs = np.arange(X_tst1.shape[1])[X_tst1[0, :] != X_tst2[0, :]]\n",
    "print len(idxs)\n",
    "idxs = idxs[:10]\n",
    "print idxs\n",
    "print X_tst1[0][idxs]\n",
    "print X_tst2[0][idxs]\n",
    "print np.sum(X_tst1[0][0:4])\n",
    "print np.sum(X_tst2[0][0:4])\n",
    "print np.sum(X_tst1[0][4:20])\n",
    "print np.sum(X_tst2[0][4:20])\n",
    "print np.sum(X_tst1[0][20:84])\n",
    "print np.sum(X_tst2[0][20:84])\n",
    "print np.sum(X_tst1[0][84:340])\n",
    "print np.sum(X_tst2[0][84:340])\n",
    "print np.sum(X_tst1[0][340:1364])\n",
    "print np.sum(X_tst2[0][340:1364])\n",
    "print np.sum(X_tst1[0][1364:5460])\n",
    "print np.sum(X_tst2[0][1364:5460])\n",
    "\n",
    "def get_feature_names(seqs, k, outfile):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('00', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seqsPath = '../features/sequences_sharpr_znormed_jul23.fa.gz'\n",
    "labelsPath = '../labels/labels_sharpr_znormed_jul23.txt.gz'\n",
    "\n",
    "trainSplitPath = '../splits/sharpr_znormed_jul23/train_split.txt.gz'\n",
    "valSplitPath = '../splits/sharpr_znormed_jul23/val_split.txt.gz'\n",
    "testSplitPath = '../splits/sharpr_znormed_jul23/test_split.txt.gz'\n",
    "\n",
    "seqs = get_seqs_from_file(seqsPath)\n",
    "labels = get_labels_from_file(labelsPath)\n",
    "\n",
    "trainSeqs, trainLabels = get_seqs_labels_from_split(trainSplitPath, seqs, labels)\n",
    "valSeqs, valLabels = get_seqs_labels_from_split(valSplitPath, seqs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idxs_without_N = [i for (i, seq) in enumerate(trainSeqs) if 'N' not in seq]\n",
    "trainSeqs = trainSeqs[train_idxs_without_N]\n",
    "trainLabels = trainLabels[train_idxs_without_N]\n",
    "\n",
    "val_idxs_without_N = [i for (i, seq) in enumerate(valSeqs) if 'N' not in seq]\n",
    "valSeqs = valSeqs[val_idxs_without_N]\n",
    "valLabels = valLabels[val_idxs_without_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(914336,) (914336, 12) (19833,) (19833, 12)\n"
     ]
    }
   ],
   "source": [
    "print trainSeqs.shape, trainLabels.shape, valSeqs.shape, valLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(914336, 5460)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainLabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7fbb14db3391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# X_train = np.array([np.ravel(util.seq_to_one_hot(seq)) for seq in trainSeqs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#[:ntest]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# X_val = get_kmer_features(valSeqs[:ntest], k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# X_val = np.array([np.ravel(util.seq_to_one_hot(seq)) for seq in valSeqs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainLabels' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from avutils import util\n",
    "\n",
    "# ntest = len(trainSeqs)\n",
    "# ntest = 10\n",
    "k = 6\n",
    "label_idx = 2 # k562_minp_norm_avg\n",
    "\n",
    "t0 = time.time()\n",
    "# X_train = get_kmer_features(trainSeqs[:ntest], k)\n",
    "# X_train = np.array([np.ravel(util.seq_to_one_hot(seq)) for seq in trainSeqs])\n",
    "print X_train.shape\n",
    "y_train = trainLabels[:, label_idx]#[:ntest]\n",
    "# X_val = get_kmer_features(valSeqs[:ntest], k)\n",
    "# X_val = np.array([np.ravel(util.seq_to_one_hot(seq)) for seq in valSeqs])\n",
    "print X_val.shape\n",
    "y_val = valLabels[:, label_idx]#[:ntest]\n",
    "print(\"Creating k-mer features for train/val set took %.3f s\" % (time.time() - t0))\n",
    "# print(\"Creating one-hot features for train/val set took %.3f s\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features take forever to generate, so just save them to file\n",
    "\n",
    "# np.savetxt(fname = '../features/xgb/upTo6Mers_train_aug1.txt',\n",
    "#            X_train,\n",
    "#            fmt = '%s',\n",
    "#            delimiter = '\\t'\n",
    "#           )\n",
    "# np.savetxt(fname = '../features/xgb/upTo6Mers_val_aug1.txt',\n",
    "#            X_val,\n",
    "#            fmt = '%s',\n",
    "#            delimiter = '\\t'\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XGB k-mer features for 914336 datapoints took 764.426 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ntest = 10000\n",
    "\n",
    "t0 = time.time()\n",
    "X_train = pd.read_csv('../features/xgb/upTo6Mers_train_aug1.txt',\n",
    "                      dtype = np.uint8,\n",
    "                      delimiter = '\\t',\n",
    "#                       nrows = ntest,\n",
    "                      header = None\n",
    "                     ).values\n",
    "X_val = pd.read_csv('../features/xgb/upTo6Mers_val_aug1.txt',\n",
    "                    dtype = np.uint8,\n",
    "                    delimiter = '\\t',\n",
    "#                     nrows = ntest,\n",
    "                    header = None\n",
    "                   ).values\n",
    "print(\"Loading XGB k-mer features for %d datapoints took %.3f s\" % (len(X_train), time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import logistic\n",
    "\n",
    "sample_weights = logistic.cdf(np.reciprocal(np.abs(trainLabels[:, 0] - trainLabels[:, 1] + 0.1)))\n",
    "print np.mean(sample_weights[:ntest]), np.std(sample_weights[:ntest])\n",
    "# sample_weights = np.ones(len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape # should be (lenX, sum i=1 to k of 4^i (k=6 --> 5460))\n",
    "print X_val.shape\n",
    "print y_train.shape\n",
    "print y_val.shape\n",
    "print np.max(X_train[0]) # should be >= ceil(145/4) = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# eval_set = [(X_val, y_val)]\n",
    "eval_dmatrix = xgb.DMatrix(data = X_val,\n",
    "                           label = y_val)\n",
    "params = {'max_depth': 6, \n",
    "          'learning_rate': 0.2,\n",
    "          'n_estimators': 250,\n",
    "          'objective': 'reg:linear',\n",
    "          'silent': 0,\n",
    "#           'updater': 'grow_gpu',\n",
    "          'random_state': 0,\n",
    "          'tree_method': 'exact',\n",
    "#           'gpu_id': 2\n",
    "          }         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def spearman_eval(y_pred, y_true):\n",
    "#     print y_true\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true.get_label())\n",
    "#     print(y_pred.shape, y_true.shape)\n",
    "    return ('spearman', spearmanr(y_pred, y_true)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "k=6\n",
    "model_path = '../model_files/xgb_kmer_sharpr_aug1/'\n",
    "os.system(\"mkdir %s\" % model_path)\n",
    "random_id = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(5))\n",
    "print(\"On training run %s\" % random_id)\n",
    "model_name = str(k) + 'merModel_record_%s_<>.model' % random_id\n",
    "# model_name = 'flattenSeq_record_%s_<>.model' % random_id\n",
    "record_number = 0\n",
    "\n",
    "# ntest = 20000\n",
    "lr_decay = 0.996\n",
    "n_batches = 1 # to avoid GPU memory errors\n",
    "ti = time.time()\n",
    "t0 = time.time()\n",
    "for i in range(n_batches):\n",
    "    # create xgboost data DMatrix objects\n",
    "    start_idx = i*len(X_train) / n_batches\n",
    "    end_idx = (i+1)*len(X_train) / n_batches\n",
    "    X_batch = X_train[start_idx : end_idx]\n",
    "    y_batch = y_train[start_idx : end_idx]\n",
    "    weights_batch = sample_weights[start_idx : end_idx]\n",
    "    batch_matrix = xgb.DMatrix(data = X_batch,\n",
    "                               label = y_batch,\n",
    "                               weight = weights_batch)\n",
    "    if i == 0:\n",
    "        bst = xgb.train(params = params,\n",
    "                        dtrain = batch_matrix,\n",
    "                        evals = [\n",
    "                                 (eval_dmatrix, 'val'),\n",
    "#                                  (batch_matrix, 'train')\n",
    "                                ],\n",
    "                        num_boost_round = 1000,\n",
    "                        feval = spearman_eval,\n",
    "                        maximize = True,\n",
    "                        early_stopping_rounds = 5,\n",
    "                        learning_rates = lambda x, y : params['learning_rate']*(lr_decay**x)\n",
    "                        )\n",
    "        bst.save_model(model_path + '/intermediate_models/intermediateModel%d_%s.model' % (i, random_id))\n",
    "        if n_batches > 1:\n",
    "            del bst\n",
    "    else:\n",
    "        # learning rate decay\n",
    "        params['learning_rate'] *= lr_decay\n",
    "        bst = xgb.train(params = params,\n",
    "                        dtrain = batch_matrix,\n",
    "                        evals = [\n",
    "                                 (eval_dmatrix, 'val'),\n",
    "#                                  (batch_matrix, 'train')\n",
    "                                ],\n",
    "                        num_boost_round = 1000,\n",
    "                        feval = spearman_eval,\n",
    "                        maximize = True,\n",
    "                        early_stopping_rounds = 5,\n",
    "                        xgb_model = model_path + '/intermediate_models/intermediateModel%d_%s.model' % (i-1, random_id)\n",
    "                        )\n",
    "        bst.save_model(model_path + '/intermediate_models/intermediateModel%d_%s.model' % (i, random_id))\n",
    "        if i != n_batches - 1:\n",
    "            del bst\n",
    "    print(\"Training model on batch %d took %.3f s\" % (i+1, time.time() - t0))\n",
    "    t0 = time.time()\n",
    "bst.save_model(model_path + '/' + model_name.replace('<>', str(record_number)))\n",
    "# model.fit(X_train[:ntest], \n",
    "#           y_train[:ntest], \n",
    "#           eval_set = eval_set, \n",
    "#           early_stopping_rounds = 8, \n",
    "#           eval_metric = 'mae',\n",
    "#           sample_weight = sample_weights[:ntest])\n",
    "print(\"Fitting model took %.3f s\" % (time.time() - ti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_val_pred = bst.predict(eval_dmatrix)\n",
    "print y_val_pred.shape\n",
    "print y_val.shape\n",
    "print spearmanr(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(y_train), np.std(y_train)\n",
    "print np.mean(y_val_pred), np.std(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_functions import jointplot\n",
    "\n",
    "jointplot(vals1 = y_val, \n",
    "          vals2 = y_val_pred,\n",
    "          out_pdf = \"../plots/sharpr_scatterplots/kmer-xgb/6mer_Unweighted_CpuExact.png\",\n",
    "          show = True,\n",
    "          cor = 'spearmanr',\n",
    "          square = True,\n",
    "          despine = False,\n",
    "          x_label = \"Sharpr Z-Score, Experimental\",\n",
    "          y_label = \"Sharpr Z-Score, Predicted\",\n",
    "          figsize = 6,\n",
    "          ratio = 6,\n",
    "          dpi = 300,\n",
    "          color = 'red',\n",
    "          kde = True,\n",
    "          bw = 'scott'\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature importances of XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.Booster(model_file = '../model_files/xgb_kmer_sharpr_aug1/6merModel_record_7VCRS_0.model')\n",
    "feature_importances = bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['f90' '9']\n",
      " ['f33' '9']\n",
      " ['f1293' '9']\n",
      " ['f26' '9']\n",
      " ['f30' '9']\n",
      " ['f22' '9']\n",
      " ['f737' '9']\n",
      " ['f436' '9']\n",
      " ['f85' '9']\n",
      " ['f1429' '8']]\n"
     ]
    }
   ],
   "source": [
    "importances = np.array([(k, feature_importances[k]) for k in feature_importances])\n",
    "importances = importances[np.argsort(importances[:, 1])[::-1]]\n",
    "print importances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
