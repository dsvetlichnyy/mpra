{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will be as follows:\n",
    "\n",
    "1. Use Peyton's model json architecture file + h5 weights file to generate predictions for all the train/val/test examples\n",
    "2. Write these predictions to a file. These will be used as labels for the MPRA model\n",
    "3. Convert sequences to 145bp by taking the central 145bp regions\n",
    "4. Using these sequences as input, train to predict accessibility outputs\n",
    "5. Initialize MPRA models with this saved architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use accessibility model architecture + weights to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/users/rmovva/anaconda2/envs/mommadragonn/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6020 on context None\n",
      "Mapped name None to device cuda: Tesla P100-PCIE-16GB (0000:03:00.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "conv1_layer0 (Convolution1D)     (None, 984, 300)      20700       convolution1d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnorm1_layer1 (BatchNormaliz (None, 984, 300)      1200        conv1_layer0[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation1_layer2 (Activation)  (None, 984, 300)      0           batchnorm1_layer1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_layer3 (Convolution1D)     (None, 978, 250)      525250      activation1_layer2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batchnorm2_layer4 (BatchNormaliz (None, 978, 250)      1000        conv2_layer3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation2_layer5 (Activation)  (None, 978, 250)      0           batchnorm2_layer4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpool1_layer6 (MaxPooling1D)   (None, 244, 250)      0           activation2_layer5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv3_layer7 (Convolution1D)     (None, 238, 250)      437750      maxpool1_layer6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnorm3_layer8 (BatchNormaliz (None, 238, 250)      1000        conv3_layer7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation3_layer9 (Activation)  (None, 238, 250)      0           batchnorm3_layer8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpool2_layer10 (MaxPooling1D)  (None, 79, 250)       0           activation3_layer9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "flatten_layer11 (Flatten)        (None, 19750)         0           maxpool2_layer10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense1_layer12 (Dense)           (None, 600)           11850600    flatten_layer11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation4_layer13 (Activation) (None, 600)           0           dense1_layer12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout1_layer14 (Dropout)       (None, 600)           0           activation4_layer13[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dense2_layer15 (Dense)           (None, 16)            9616        dropout1_layer14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation5_layer16 (Activation) (None, 16)            0           dense2_layer15[0][0]             \n",
      "====================================================================================================\n",
      "Total params: 12,847,116\n",
      "Trainable params: 12,845,516\n",
      "Non-trainable params: 1,600\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# json_path = \"../model_files/atac_xferlearn_jul24/record_12_model_FbFup_modelJson.json\"\n",
    "# json_path = \"../model_files/sharpr_znormed_jul23/record_13_model_bgGhy_modelJson.json\"\n",
    "# json_path = \"../model_files/regressionJun24Positives/record_2_model_Yjv2n_modelJson.json\"\n",
    "json_path = \"../model_files/atac_xferlearn_jul24/transferlearn_json_keras1.json\"\n",
    "# json_path = \"../model_files/atac_xferlearn_jul24/record_3_model_RatMV_modelJson.json\"\n",
    "with open(json_path) as json_file:\n",
    "    json_string = json.dumps(json.load(json_file))\n",
    "    model = model_from_json(json_string) \n",
    "    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# (300, 4, 17, 1) --> (17, 1, 4, 300)\n",
    "# swap 0,3; 1,2; 0,1: 0, 1, 2, 3 --> 3, 1, 2, 0 --> 3, 2, 1, 0 --> 2, 3, 1, 0\n",
    "\n",
    "f = h5py.File('../model_files/atac_xferlearn_jul24/record_12_model_FbFup_modelWeights.h5', 'r')\n",
    "\n",
    "# Checking to see if shapes match up\n",
    "# # print f.keys()\n",
    "# # print model.layers[0].name\n",
    "# print \"Previous model weight shapes\"\n",
    "# for i in range(len(f.keys())):\n",
    "#     layer = 'layer_%d' % i\n",
    "# #     print layer\n",
    "# #     print f[layer].keys()\n",
    "#     for params in f[layer].keys():\n",
    "#         print f[layer][params].shape\n",
    "\n",
    "# print \"\\nNew model weight shapes\"\n",
    "# for w in model.get_weights():\n",
    "#     print w.shape\n",
    "\n",
    "weights = []\n",
    "for i in range(len(f.keys())):\n",
    "    layer = 'layer_%d' % i\n",
    "#     if len(f[layer].keys()) == 4:\n",
    "#         batchnorm_params = f[layer].keys()\n",
    "#         print batchnorm_params\n",
    "# #         print f[layer][batchnorm_params[2]].shape\n",
    "#         weights.append(f[layer][batchnorm_params[0]])\n",
    "#         weights.append(f[layer][batchnorm_params[1]])\n",
    "#         weights.append(f[layer][batchnorm_params[2]])\n",
    "#         weights.append(f[layer][batchnorm_params[3]])\n",
    "#         continue\n",
    "    for params in f[layer].keys():\n",
    "        layer_W = np.array(f[layer][params])\n",
    "        if len(layer_W.shape) == 4:\n",
    "            layer_W = np.swapaxes(np.swapaxes(np.swapaxes(layer_W, 0, 3), 1, 2), 0, 1)\n",
    "        weights.append(layer_W)\n",
    "\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 1676\n",
      "Batches 0 to 100 took 66.978\n",
      "On batch 100\n",
      "Batches 100 to 200 took 68.213\n",
      "On batch 200\n",
      "Batches 200 to 300 took 68.261\n",
      "On batch 300\n",
      "Batches 300 to 400 took 67.920\n",
      "On batch 400\n",
      "Batches 400 to 500 took 67.196\n",
      "On batch 500\n",
      "Batches 500 to 600 took 67.861\n",
      "On batch 600\n",
      "Batches 600 to 700 took 67.277\n",
      "On batch 700\n",
      "Batches 700 to 800 took 68.547\n",
      "On batch 800\n",
      "Batches 800 to 900 took 68.343\n",
      "On batch 900\n",
      "Batches 900 to 1000 took 66.979\n",
      "On batch 1000\n",
      "Batches 1000 to 1100 took 66.711\n",
      "On batch 1100\n",
      "Batches 1100 to 1200 took 71.508\n",
      "On batch 1200\n",
      "Batches 1200 to 1300 took 68.118\n",
      "On batch 1300\n",
      "Batches 1300 to 1400 took 67.790\n",
      "On batch 1400\n",
      "Batches 1400 to 1500 took 70.220\n",
      "On batch 1500\n",
      "Batches 1500 to 1600 took 68.200\n",
      "On batch 1600\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "train_data = h5py.File(\"../hdf5files/atac_xferlearn_jul24/train_data.hdf5\")\n",
    "X_train = train_data['X']['sequence']\n",
    "y_train_pred = np.ndarray(shape = (len(X_train), 16))\n",
    "\n",
    "batch_size = 500\n",
    "t0 = time.time()\n",
    "print \"Total batches: %d\" % (len(X_train)/batch_size + 1)\n",
    "for i in range(len(X_train)/batch_size + 1):\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        print(\"Batches %d to %d took %.3f sec\" % (i-100, i, time.time() - t0))\n",
    "        t0 = time.time()\n",
    "        print \"On batch %d\" % i\n",
    "    if (i+1)*batch_size > len(X_train):\n",
    "        y_train_pred[i*batch_size:] = model.predict_on_batch(X_train[i*batch_size:])\n",
    "    y_train_pred[i*batch_size : (i+1)*batch_size] = model.predict_on_batch(X_train[i*batch_size : (i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.stats import spearmanr\n",
    "import time\n",
    "\n",
    "def generate_predictions(model, X, ntasks, batchsize=500):\n",
    "    y_pred = np.ndarray(shape = (len(X), ntasks))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print \"Total batches: %d\" % (len(X)/batchsize + 1)\n",
    "    for i in range(len(X)/batchsize + 1):\n",
    "        if (i % 100 == 0 or i == len(X)/batchsize) and i > 0:\n",
    "            print(\"Batches %d to %d took %.3f sec\" % (i-100, i, time.time() - t0))\n",
    "            t0 = time.time()\n",
    "            print \"On batch %d\" % i\n",
    "        if (i+1)*batchsize > len(X):\n",
    "            y_pred[i*batchsize:] = model.predict_on_batch(X[i*batchsize:])\n",
    "        y_pred[i*batchsize : (i+1)*batchsize] = model.predict_on_batch(X[i*batchsize : (i+1)*batchsize])\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, ntasks):\n",
    "    for i in range(ntasks):\n",
    "        y_true_task = y_true[:, i]\n",
    "        y_pred_task = y_pred[:, i]\n",
    "        auroc = roc_auc_score(y_true_task, y_pred_task)\n",
    "        auprc = average_precision_score(y_true_task, y_pred_task)\n",
    "        sprmn = spearmanr(y_true_task, y_pred_task)\n",
    "        print \"Task %d: AUROC = %.3f, AUPRC = %.3f, Spearman = %.3f, p = %.3f\" % (i, auroc, auprc, sprmn[0], sprmn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 2\n",
      "Batches -99 to 1 took 0.384 sec\n",
      "On batch 1\n"
     ]
    }
   ],
   "source": [
    "val_data = h5py.File(\"../hdf5files/atac_xferlearn_jul24/valid_data.hdf5\")\n",
    "X_val = val_data['X']['sequence'][:600]\n",
    "\n",
    "y_pred_val = generate_predictions(model, X_val, batchsize = 500, ntasks = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.0\n",
      "1.19464308713e-25\n",
      "\n",
      "Validation set evaluation\n",
      "Task 0: AUROC = 0.496, AUPRC = 0.307, Spearman = -0.047, p = 0.249\n",
      "Task 1: AUROC = 0.500, AUPRC = 0.240, Spearman = nan, p = nan\n",
      "Task 2: AUROC = 0.533, AUPRC = 0.379, Spearman = 0.055, p = 0.182\n",
      "Task 3: AUROC = 0.499, AUPRC = 0.163, Spearman = -0.018, p = 0.659\n",
      "Task 4: AUROC = 0.466, AUPRC = 0.117, Spearman = -0.067, p = 0.102\n",
      "Task 5: AUROC = 0.500, AUPRC = 0.055, Spearman = nan, p = nan\n",
      "Task 6: AUROC = 0.629, AUPRC = 0.200, Spearman = 0.155, p = 0.000\n",
      "Task 7: AUROC = 0.483, AUPRC = 0.251, Spearman = -0.077, p = 0.058\n",
      "Task 8: AUROC = 0.438, AUPRC = 0.141, Spearman = -0.102, p = 0.012\n",
      "Task 9: AUROC = 0.510, AUPRC = 0.258, Spearman = 0.021, p = 0.606\n",
      "Task 10: AUROC = 0.579, AUPRC = 0.406, Spearman = 0.126, p = 0.002\n",
      "Task 11: AUROC = 0.500, AUPRC = 0.303, Spearman = nan, p = nan\n",
      "Task 12: AUROC = 0.484, AUPRC = 0.087, Spearman = -0.053, p = 0.198\n",
      "Task 13: AUROC = 0.596, AUPRC = 0.190, Spearman = 0.116, p = 0.005\n",
      "Task 14: AUROC = 0.500, AUPRC = 0.323, Spearman = nan, p = nan\n",
      "Task 15: AUROC = 0.500, AUPRC = 0.338, Spearman = nan, p = nan\n"
     ]
    }
   ],
   "source": [
    "# y_train_true = train_data['Y']['output']\n",
    "y_val_true = val_data['Y']['output'][:600]\n",
    "print np.sum(y_val_true[:, 0])\n",
    "print np.sum(y_pred_val[:, 0])\n",
    "\n",
    "# print \"Training set evaluation\"\n",
    "# evaluate_predictions(y_train_true, y_train_pred, ntasks=16)\n",
    "print \"\\nValidation set evaluation\"\n",
    "evaluate_predictions(y_val_true, y_pred_val, ntasks=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509535074234\n"
     ]
    }
   ],
   "source": [
    "# On cpu, ~30s for 15 sample batch. On gpu, 0.5 sec.\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "y = model.predict_on_batch(X_train[i*batch_size : (i+1)*batch_size])\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the parameter initialization did not seem to work. The model just kept outputting full zeros for all the inputs, which was unfortunate. Not sure why - I thought it had something to do with the batch norm, but it looks like that's not it. So, I'm just going to create 145bp input sequences and train the MPRA model directly on the accessibility data (as opposed to training on the outputs of the accessibility model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert train/val/test HDF5 files to 145bp inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example was previously 1000bp; I'm going to take the $145*5 = 725$ central base pairs in each sequence, and use those as 5 separate training examples. Performance will probably be lower since accessibility probably does require more context than just 145bp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138 283 428 573 718]\n",
      "On file ../hdf5files/atac_xferlearn_jul24/pretrain_valid_data.hdf5\n",
      "(41635, 1000, 4)\n",
      "(208175, 145, 4)\n",
      "(41635, 16)\n",
      "(208175, 16)\n",
      "On file ../hdf5files/atac_xferlearn_jul24/pretrain_train_data.hdf5\n",
      "(837977, 1000, 4)\n",
      "(4189885, 145, 4)\n",
      "(837977, 16)\n",
      "(4189885, 16)\n",
      "On file ../hdf5files/atac_xferlearn_jul24/pretrain_test_data.hdf5\n",
      "(154967, 1000, 4)\n",
      "(774835, 145, 4)\n",
      "(154967, 16)\n",
      "(774835, 16)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "fnames = ['valid_data.hdf5', 'train_data.hdf5', 'test_data.hdf5']\n",
    "fnames = ['../hdf5files/atac_xferlearn_jul24/pretrain_' + name for name in fnames]\n",
    "windows_per_seq = 5\n",
    "seqlen = 145\n",
    "start_indices = np.arange(500 - seqlen*windows_per_seq/2, 500 + seqlen*windows_per_seq/2, seqlen)\n",
    "print start_indices\n",
    "for fname in fnames:\n",
    "    print \"On file %s\" % fname\n",
    "    f = h5py.File(fname, 'r+')\n",
    "    \n",
    "    # Sequences\n",
    "    sequences = np.array(f['X/sequence'])\n",
    "    print sequences.shape\n",
    "    new_sequences = np.ndarray(shape = (windows_per_seq*len(sequences), seqlen, 4))\n",
    "    for i in range(windows_per_seq):\n",
    "        new_sequences[np.arange(i, len(new_sequences), windows_per_seq)] = sequences[:, start_indices[i] : start_indices[i] + 145]\n",
    "    print new_sequences.shape\n",
    "    del f['X/sequence']\n",
    "    f.create_dataset('X/sequence', data = new_sequences)\n",
    "    \n",
    "    # Labels\n",
    "    labels = np.array(f['Y/output'])\n",
    "    print labels.shape\n",
    "    new_labels = np.repeat(labels, windows_per_seq, axis=0)\n",
    "    print new_labels.shape\n",
    "    del f['Y/output']\n",
    "    f.create_dataset('Y/output', data = new_labels)\n",
    "    \n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208175, 145, 4)\n",
      "(208175, 16)\n",
      "[[ 1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.]\n",
      " [ 1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('../hdf5files/atac_xferlearn_jul24/pretrain_valid_data.hdf5')\n",
    "print f['X/sequence'].shape\n",
    "print f['Y/output'].shape\n",
    "print f['Y/output'][5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate predictions of ensembled MPRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Load architecture from any one of the many JSON files\n",
    "json_path = \"../model_files/sharpr_znormed_jul23/record_13_model_bgGhy_modelJson.json\"\n",
    "with open(json_path) as json_file:\n",
    "    json_string = json.dumps(json.load(json_file))\n",
    "    model = model_from_json(json_string) \n",
    "    \n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_files = [\"../model_files/sharpr_znormed_jul23//record_13_model_bgGhy_modelWeights.h5\", # 0.191\n",
    "                 \"../model_files/sharpr_znormed_jul23//record_14_model_10Sx5_modelWeights.h5\", # 0.186\n",
    "                 \n",
    "                ]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mommadragonn]",
   "language": "python",
   "name": "conda-env-mommadragonn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
